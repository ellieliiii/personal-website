"use strict";
/*
 * ATTENTION: An "eval-source-map" devtool has been used.
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file with attached SourceMaps in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
self["webpackHotUpdate_N_E"]("app/projects/page",{

/***/ "(app-pages-browser)/./src/data/projects/content/llm-coding-eval.ts":
/*!******************************************************!*\
  !*** ./src/data/projects/content/llm-coding-eval.ts ***!
  \******************************************************/
/***/ (function(module, __webpack_exports__, __webpack_require__) {

eval(__webpack_require__.ts("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   content: function() { return /* binding */ content; },\n/* harmony export */   description: function() { return /* binding */ description; }\n/* harmony export */ });\nconst description = \"\\nCan inserted errors improve the performance of LLMs? This project investigates the impact of inserting errors in proposed coding solutions and if the performance of LLMs can be improved by doing so.\\n\";\nconst content = '\\n<h3>Introduction</h2>\\n\\n<p>Large Language Models (LLMs), such as GPT-3.5 and GPT-4, have recently shown significant progress in their code generation abilities. These models can take in natural language instructions and generate code that meets the user\\'s instructions. However, the code generated by these models is often incorrect, due to both logical and syntactical errors. This project introduces a novel prompt engineering technique, <em>use mistake</em>, which first asks the LLM to construct an erroneous response to the coding problem. Then, that code is passed along with the original prompt to construct the final response. To benchmark the prompting techniques, two code evaluation datasets were used: HumanEval and PythonSaga. The <em>use mistake</em> technique achieved better performance than the standard coding attempt benchmark on the HumanEval dataset with 10 passes. An exploratory analysis was also conducted on how different prompting methods fail specific coding problems.</p>\\n\\n<h3>Data</h2>\\n\\n<p>Two different datasets were used to evaluate the effects of various prompting techniques on LLM code generation. The first dataset was <a href=\"https://github.com/openai/human-eval\">HumanEval</a>, a code evaluation dataset that contains 164 hand-crafted Python basic programming problems, designed to ensure that LLMs were not directly trained on these problems. Each of these problems contains a prompt with the method header and example inputs/outputs, along with an example solution and unit tests for the generated code. A significant benefit of using HumanEval is that the code automating the testing of LLM-generated outputs is publicly available. Since its creation in 2021, HumanEval has been widely used to evaluate and test code generation models. The second dataset, <a href=\"https://anonymous.4open.science/r/PythonSaga\">PythonSaga</a>, was recently released in 2024 and contains 185 Python programming problems. These problems were curated from popular coding platforms Geek-for-Geeks and LeetCode. The authors reformulated the problems taken from these coding platforms to maintain the same underlying functionality while transforming the prompting text to make it harder for LLMs to pattern-match the prompt, forcing them to recognize the underlying concepts in different contexts.</p>\\n\\n<h3>Methodology</h2>\\n\\n<img src=\"/projects/llm-coding-eval/proposal.jpg\" alt=\"Design\">\\n\\n<p>This section describes the four different prompting methods tested with LLM code generation.</p>\\n\\n<h4>Attempt</h3>\\n<p>The first prompting method was passing the raw prompt of the coding task from the dataset in use. The purpose of this attempt is to act as a baseline to compare the performance of our other prompting methods to. The code generated by this raw attempt is also used in our second technique.</p>\\n\\n<h4>Use Attempt</h3>\\n<p>The next prompting method is the <em>use attempt</em> method which provides the LLM with an example LLM generated solution when asking it to solve the same problem. The direct prompt used was:</p>\\n\\n<blockquote>This is an attempt to the function: \\\\n {problem} \\\\n {attempt} \\\\n If the solution is correct please output the existing function code. If the solution is incorrect fix and output the function code. \\\\n {problem}</blockquote>\\n\\n<p>{problem} contains the raw prompt and {attempt} contains the output of a previous raw attempt. The idea of this strategy was to see if providing the LLM with an example solution could improve its accuracy/identification of errors in the previous attempt\\'s strategy.</p>\\n\\n<h4>Mistake</h3>\\n<p>Our next prompting method was the mistake method. This is where we asked the LLM to incorrectly answer the coding problem that we provided it. The prompt we used was:</p>\\n\\n<blockquote>Complete this task with mistakes. Only return your addition to the existing code. Do not repeat the function header. \\\\n {problem}.</blockquote>\\n\\n<p>The purpose of this method is to be used in the use mistake prompting method, which requires an example erroneous LLM output.</p>\\n\\n<h4>Use Mistake</h3>\\n<p>The final prompting method is our novel <em>use mistake</em> method. In this method, we passed an erroneous example LLM generated solution along with the problem. The prompt used was:</p>\\n\\n<blockquote>This is an attempt to the function: \\\\n {problem} \\\\n {attempt} \\\\n If the solution is correct please output the existing function code. If the solution is incorrect fix and output the function code. \\\\n {problem}.</blockquote>\\n\\n<p>The intention behind this method is to prompt the LLM to be more cognizant and mindful of errors, such as the ones in the wrong attempt that they are passed. We hoped that this would then correlate to reduced number of errors and better generated solutions.</p>\\n\\n<h3>Evaluation</h2>\\n\\n<p>We used both HumanEval and PythonSaga to evaluate our code prompting methods. The evaluation method used in the HumanEval paper, pass @ k, has become the standard for measuring LLM code generation accuracy. This strategy involves passing the same prompt to the LLM k times. If any of the k generations pass all of the unit tests, then this problem is given a score of \"1\" for solving the problem. Otherwise, it is given a score of \"0\". Calculating the mean of these scores across the code evaluation dataset that is being used then yields the final score. We calculated pass @ 1, 5, and 10 for all of the different prompting methods. Furthermore, we propose a novel combined metric that shows the overall combined power of the <em>attempt</em> and  <em>use mistake</em> methods. Simply, this metric checks if either one of these prompting methods yielded a correct output.</p>\\n\\n<h3>Implementation Details</h2>\\n\\n<p>For our LLM, we use OpenAI\\'s GPT-3.5 Turbo with a temperature of 0.8, as that is the standard temperature used with HumanEval. We chose GPT-3.5 Turbo due to significantly lower associated costs for repeated LLM passes with different prompts and methods and GPT-4 Turbo already having the best performance on the HumanEval dataset. We hoped to increase performance with a worse but much cheaper model to show the benefits of our prompting technique. After 10 passes for each problem, we ran the generated code in a Docker container to benchmark using the unit tests and store the results. We also stored the individual LLM generated outputs and benchmark results for more detailed output analysis. Check out our code <a href=\"https://github.com/mchales/llm-coding-eval\">here</a>.</p>\\n\\n<h3>Results</h2>\\n\\n<p>Figure 1</p>\\n<img src=\"/projects/llm-coding-eval/figure1.png\" alt=\"Figure 1\"  width=\"500\">\\n\\n<p>Figure 2</p>\\n<img src=\"/projects/llm-coding-eval/figure2.png\" alt=\"Figure 2\"  width=\"500\">\\n\\n<p>Figure 3</p>\\n<img src=\"/projects/llm-coding-eval/figure3.png\" alt=\"Figure 3\"  width=\"500\">\\n\\n<p>Looking at Figure 1 and Figure 2, we can clearly see that the LLM performs significantly better on HumanEval than PythonSaga. This makes sense as PythonSaga is much newer, and thus GPT-3.5 has had less exposure to its problems. Furthermore, as the authors of PythonSaga transformed the coding prompts to be harder for LLMs to identify the underlying question/function of the problem, it makes sense that all prompting methods perform significantly worse on it. For HumanEval, the <em>use mistake</em> prompting method had the best performance, while for PythonSaga, the <em>attempt</em> method performed the best. Given that PythonSaga\\'s involved preprocessing result in significantly poorer overall results, and HumanEval is the more tested dataset, we conducted most of our analysis with HumanEval.</p>\\n\\n<p>Interestingly, the <em>attempt</em> and <em>use mistake</em> methods don\\'t fail the same problems. We can combine their results to create the metric <em>Combined</em> for HumanEval seen in Figure 3.</p>\\n\\n<h3>Code Example</h3>\\n<p>One problem which <em>use mistake</em> succeeded while <em>attempt</em> did not was:</p>\\n\\n<blockquote>\"How many 7s are in numbers that are divisible by 11 or 13 between 1 to n, where n is an input passed in.\"</blockquote>\\n\\n<p>The <em>attempt</em> method fails to understand that numbers can have multiple 7s, and that these need to be counted individually. An example generation is:</p>\\n\\n<pre><code>count = 0\\nfor i in range(1, n):\\n    if i % 11 == 0 or i % 13 == 0:\\n        if \\'7\\' in str(i):\\n            count += 1\\nreturn count\\n</code></pre>\\n\\n<p><em>use mistake</em> successfully answered this question with the generation:</p>\\n\\n<pre><code>count = 0\\nfor i in range(1, n):\\n    if i % 11 == 0 or i % 13 == 0:\\n        for digit in str(i):\\n            if digit == \\'7\\':\\n                count += 1\\nreturn count\\n</code></pre>\\n\\n<p>Interestingly, the <em>mistake</em> generated code for this solution is not close to the correct solution:</p>\\n\\n<pre><code>if i % 7 == 0:\\n    count += 1\\ncontinue\\nreturn count\\n</code></pre>\\n\\n<p>Although not always reliable, mistake-generated outputs can influence LLMs to solve problems they previously could not.</p>\\n\\n<h3>Conclusion BLAHBLAH</h2>\\n\\n<p>This project proposes a new prompting technique for LLMs that resulted in improved performance with HumanEval pass @ 10. More research will need to be done to explore repeatability and reliability of this technique.</p>\\n\\n<h3>Extra</h3>\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th>Prompting Technique</th>\\n      <th>HumanEval Pass@1</th>\\n      <th>HumanEval Pass@5</th>\\n      <th>HumanEval Pass@10</th>\\n      <th>PythonSaga Pass@1</th>\\n      <th>PythonSaga Pass@5</th>\\n      <th>PythonSaga Pass@10</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Attempt</td>\\n      <td><strong>71.28</strong></td>\\n      <td>84.85</td>\\n      <td>89.02</td>\\n      <td><strong>18.81</strong></td>\\n      <td><strong>31.32</strong></td>\\n      <td><strong>35.68</strong></td>\\n    </tr>\\n    <tr>\\n      <td>Mistake</td>\\n      <td>10.06</td>\\n      <td>35.80</td>\\n      <td>53.05</td>\\n      <td>1.68</td>\\n      <td>7.46</td>\\n      <td>12.97</td>\\n    </tr>\\n    <tr>\\n      <td>Use Mistake</td>\\n      <td>70.79</td>\\n      <td><strong>88.41</strong></td>\\n      <td><strong>91.46</strong></td>\\n      <td>16.00</td>\\n      <td>27.57</td>\\n      <td>32.43</td>\\n    </tr>\\n    <tr>\\n      <td>Use Attempt</td>\\n      <td>66.65</td>\\n      <td>84.88</td>\\n      <td>87.80</td>\\n      <td>13.68</td>\\n      <td>24.49</td>\\n      <td>27.03</td>\\n    </tr>\\n  </tbody>\\n</table>\\n';\n\n\n;\n    // Wrapped in an IIFE to avoid polluting the global scope\n    ;\n    (function () {\n        var _a, _b;\n        // Legacy CSS implementations will `eval` browser code in a Node.js context\n        // to extract CSS. For backwards compatibility, we need to check we're in a\n        // browser context before continuing.\n        if (typeof self !== 'undefined' &&\n            // AMP / No-JS mode does not inject these helpers:\n            '$RefreshHelpers$' in self) {\n            // @ts-ignore __webpack_module__ is global\n            var currentExports = module.exports;\n            // @ts-ignore __webpack_module__ is global\n            var prevSignature = (_b = (_a = module.hot.data) === null || _a === void 0 ? void 0 : _a.prevSignature) !== null && _b !== void 0 ? _b : null;\n            // This cannot happen in MainTemplate because the exports mismatch between\n            // templating and execution.\n            self.$RefreshHelpers$.registerExportsForReactRefresh(currentExports, module.id);\n            // A module can be accepted automatically based on its exports, e.g. when\n            // it is a Refresh Boundary.\n            if (self.$RefreshHelpers$.isReactRefreshBoundary(currentExports)) {\n                // Save the previous exports signature on update so we can compare the boundary\n                // signatures. We avoid saving exports themselves since it causes memory leaks (https://github.com/vercel/next.js/pull/53797)\n                module.hot.dispose(function (data) {\n                    data.prevSignature =\n                        self.$RefreshHelpers$.getRefreshBoundarySignature(currentExports);\n                });\n                // Unconditionally accept an update to this module, we'll check if it's\n                // still a Refresh Boundary later.\n                // @ts-ignore importMeta is replaced in the loader\n                module.hot.accept();\n                // This field is set when the previous version of this module was a\n                // Refresh Boundary, letting us know we need to check for invalidation or\n                // enqueue an update.\n                if (prevSignature !== null) {\n                    // A boundary can become ineligible if its exports are incompatible\n                    // with the previous exports.\n                    //\n                    // For example, if you add/remove/change exports, we'll want to\n                    // re-execute the importing modules, and force those components to\n                    // re-render. Similarly, if you convert a class component to a\n                    // function, we want to invalidate the boundary.\n                    if (self.$RefreshHelpers$.shouldInvalidateReactRefreshBoundary(prevSignature, self.$RefreshHelpers$.getRefreshBoundarySignature(currentExports))) {\n                        module.hot.invalidate();\n                    }\n                    else {\n                        self.$RefreshHelpers$.scheduleUpdate();\n                    }\n                }\n            }\n            else {\n                // Since we just executed the code for the module, it's possible that the\n                // new exports made it ineligible for being a boundary.\n                // We only care about the case when we were _previously_ a boundary,\n                // because we already accepted this update (accidental side effect).\n                var isNoLongerABoundary = prevSignature !== null;\n                if (isNoLongerABoundary) {\n                    module.hot.invalidate();\n                }\n            }\n        }\n    })();\n//# sourceURL=[module]\n//# sourceMappingURL=data:application/json;charset=utf-8;base64,{"version":3,"file":"(app-pages-browser)/./src/data/projects/content/llm-coding-eval.ts","mappings":";;;;;AAAO,MACPA,cAAe,6MAEb;AAEK,MAAMC,UAAW,iwUA2JtB","sources":["webpack://_N_E/./src/data/projects/content/llm-coding-eval.ts?d701"],"sourcesContent":["export const \ndescription = `\nCan inserted errors improve the performance of LLMs? This project investigates the impact of inserting errors in proposed coding solutions and if the performance of LLMs can be improved by doing so.\n`;\n\nexport const content = `\n<h3>Introduction</h2>\n\n<p>Large Language Models (LLMs), such as GPT-3.5 and GPT-4, have recently shown significant progress in their code generation abilities. These models can take in natural language instructions and generate code that meets the user's instructions. However, the code generated by these models is often incorrect, due to both logical and syntactical errors. This project introduces a novel prompt engineering technique, <em>use mistake</em>, which first asks the LLM to construct an erroneous response to the coding problem. Then, that code is passed along with the original prompt to construct the final response. To benchmark the prompting techniques, two code evaluation datasets were used: HumanEval and PythonSaga. The <em>use mistake</em> technique achieved better performance than the standard coding attempt benchmark on the HumanEval dataset with 10 passes. An exploratory analysis was also conducted on how different prompting methods fail specific coding problems.</p>\n\n<h3>Data</h2>\n\n<p>Two different datasets were used to evaluate the effects of various prompting techniques on LLM code generation. The first dataset was <a href=\"https://github.com/openai/human-eval\">HumanEval</a>, a code evaluation dataset that contains 164 hand-crafted Python basic programming problems, designed to ensure that LLMs were not directly trained on these problems. Each of these problems contains a prompt with the method header and example inputs/outputs, along with an example solution and unit tests for the generated code. A significant benefit of using HumanEval is that the code automating the testing of LLM-generated outputs is publicly available. Since its creation in 2021, HumanEval has been widely used to evaluate and test code generation models. The second dataset, <a href=\"https://anonymous.4open.science/r/PythonSaga\">PythonSaga</a>, was recently released in 2024 and contains 185 Python programming problems. These problems were curated from popular coding platforms Geek-for-Geeks and LeetCode. The authors reformulated the problems taken from these coding platforms to maintain the same underlying functionality while transforming the prompting text to make it harder for LLMs to pattern-match the prompt, forcing them to recognize the underlying concepts in different contexts.</p>\n\n<h3>Methodology</h2>\n\n<img src=\"/projects/llm-coding-eval/proposal.jpg\" alt=\"Design\">\n\n<p>This section describes the four different prompting methods tested with LLM code generation.</p>\n\n<h4>Attempt</h3>\n<p>The first prompting method was passing the raw prompt of the coding task from the dataset in use. The purpose of this attempt is to act as a baseline to compare the performance of our other prompting methods to. The code generated by this raw attempt is also used in our second technique.</p>\n\n<h4>Use Attempt</h3>\n<p>The next prompting method is the <em>use attempt</em> method which provides the LLM with an example LLM generated solution when asking it to solve the same problem. The direct prompt used was:</p>\n\n<blockquote>This is an attempt to the function: \\\\n {problem} \\\\n {attempt} \\\\n If the solution is correct please output the existing function code. If the solution is incorrect fix and output the function code. \\\\n {problem}</blockquote>\n\n<p>{problem} contains the raw prompt and {attempt} contains the output of a previous raw attempt. The idea of this strategy was to see if providing the LLM with an example solution could improve its accuracy/identification of errors in the previous attempt's strategy.</p>\n\n<h4>Mistake</h3>\n<p>Our next prompting method was the mistake method. This is where we asked the LLM to incorrectly answer the coding problem that we provided it. The prompt we used was:</p>\n\n<blockquote>Complete this task with mistakes. Only return your addition to the existing code. Do not repeat the function header. \\\\n {problem}.</blockquote>\n\n<p>The purpose of this method is to be used in the use mistake prompting method, which requires an example erroneous LLM output.</p>\n\n<h4>Use Mistake</h3>\n<p>The final prompting method is our novel <em>use mistake</em> method. In this method, we passed an erroneous example LLM generated solution along with the problem. The prompt used was:</p>\n\n<blockquote>This is an attempt to the function: \\\\n {problem} \\\\n {attempt} \\\\n If the solution is correct please output the existing function code. If the solution is incorrect fix and output the function code. \\\\n {problem}.</blockquote>\n\n<p>The intention behind this method is to prompt the LLM to be more cognizant and mindful of errors, such as the ones in the wrong attempt that they are passed. We hoped that this would then correlate to reduced number of errors and better generated solutions.</p>\n\n<h3>Evaluation</h2>\n\n<p>We used both HumanEval and PythonSaga to evaluate our code prompting methods. The evaluation method used in the HumanEval paper, pass @ k, has become the standard for measuring LLM code generation accuracy. This strategy involves passing the same prompt to the LLM k times. If any of the k generations pass all of the unit tests, then this problem is given a score of \"1\" for solving the problem. Otherwise, it is given a score of \"0\". Calculating the mean of these scores across the code evaluation dataset that is being used then yields the final score. We calculated pass @ 1, 5, and 10 for all of the different prompting methods. Furthermore, we propose a novel combined metric that shows the overall combined power of the <em>attempt</em> and  <em>use mistake</em> methods. Simply, this metric checks if either one of these prompting methods yielded a correct output.</p>\n\n<h3>Implementation Details</h2>\n\n<p>For our LLM, we use OpenAI's GPT-3.5 Turbo with a temperature of 0.8, as that is the standard temperature used with HumanEval. We chose GPT-3.5 Turbo due to significantly lower associated costs for repeated LLM passes with different prompts and methods and GPT-4 Turbo already having the best performance on the HumanEval dataset. We hoped to increase performance with a worse but much cheaper model to show the benefits of our prompting technique. After 10 passes for each problem, we ran the generated code in a Docker container to benchmark using the unit tests and store the results. We also stored the individual LLM generated outputs and benchmark results for more detailed output analysis. Check out our code <a href=\"https://github.com/mchales/llm-coding-eval\">here</a>.</p>\n\n<h3>Results</h2>\n\n<p>Figure 1</p>\n<img src=\"/projects/llm-coding-eval/figure1.png\" alt=\"Figure 1\"  width=\"500\">\n\n<p>Figure 2</p>\n<img src=\"/projects/llm-coding-eval/figure2.png\" alt=\"Figure 2\"  width=\"500\">\n\n<p>Figure 3</p>\n<img src=\"/projects/llm-coding-eval/figure3.png\" alt=\"Figure 3\"  width=\"500\">\n\n<p>Looking at Figure 1 and Figure 2, we can clearly see that the LLM performs significantly better on HumanEval than PythonSaga. This makes sense as PythonSaga is much newer, and thus GPT-3.5 has had less exposure to its problems. Furthermore, as the authors of PythonSaga transformed the coding prompts to be harder for LLMs to identify the underlying question/function of the problem, it makes sense that all prompting methods perform significantly worse on it. For HumanEval, the <em>use mistake</em> prompting method had the best performance, while for PythonSaga, the <em>attempt</em> method performed the best. Given that PythonSaga's involved preprocessing result in significantly poorer overall results, and HumanEval is the more tested dataset, we conducted most of our analysis with HumanEval.</p>\n\n<p>Interestingly, the <em>attempt</em> and <em>use mistake</em> methods don't fail the same problems. We can combine their results to create the metric <em>Combined</em> for HumanEval seen in Figure 3.</p>\n\n<h3>Code Example</h3>\n<p>One problem which <em>use mistake</em> succeeded while <em>attempt</em> did not was:</p>\n\n<blockquote>\"How many 7s are in numbers that are divisible by 11 or 13 between 1 to n, where n is an input passed in.\"</blockquote>\n\n<p>The <em>attempt</em> method fails to understand that numbers can have multiple 7s, and that these need to be counted individually. An example generation is:</p>\n\n<pre><code>count = 0\nfor i in range(1, n):\n    if i % 11 == 0 or i % 13 == 0:\n        if '7' in str(i):\n            count += 1\nreturn count\n</code></pre>\n\n<p><em>use mistake</em> successfully answered this question with the generation:</p>\n\n<pre><code>count = 0\nfor i in range(1, n):\n    if i % 11 == 0 or i % 13 == 0:\n        for digit in str(i):\n            if digit == '7':\n                count += 1\nreturn count\n</code></pre>\n\n<p>Interestingly, the <em>mistake</em> generated code for this solution is not close to the correct solution:</p>\n\n<pre><code>if i % 7 == 0:\n    count += 1\ncontinue\nreturn count\n</code></pre>\n\n<p>Although not always reliable, mistake-generated outputs can influence LLMs to solve problems they previously could not.</p>\n\n<h3>Conclusion BLAHBLAH</h2>\n\n<p>This project proposes a new prompting technique for LLMs that resulted in improved performance with HumanEval pass @ 10. More research will need to be done to explore repeatability and reliability of this technique.</p>\n\n<h3>Extra</h3>\n\n<table>\n  <thead>\n    <tr>\n      <th>Prompting Technique</th>\n      <th>HumanEval Pass@1</th>\n      <th>HumanEval Pass@5</th>\n      <th>HumanEval Pass@10</th>\n      <th>PythonSaga Pass@1</th>\n      <th>PythonSaga Pass@5</th>\n      <th>PythonSaga Pass@10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Attempt</td>\n      <td><strong>71.28</strong></td>\n      <td>84.85</td>\n      <td>89.02</td>\n      <td><strong>18.81</strong></td>\n      <td><strong>31.32</strong></td>\n      <td><strong>35.68</strong></td>\n    </tr>\n    <tr>\n      <td>Mistake</td>\n      <td>10.06</td>\n      <td>35.80</td>\n      <td>53.05</td>\n      <td>1.68</td>\n      <td>7.46</td>\n      <td>12.97</td>\n    </tr>\n    <tr>\n      <td>Use Mistake</td>\n      <td>70.79</td>\n      <td><strong>88.41</strong></td>\n      <td><strong>91.46</strong></td>\n      <td>16.00</td>\n      <td>27.57</td>\n      <td>32.43</td>\n    </tr>\n    <tr>\n      <td>Use Attempt</td>\n      <td>66.65</td>\n      <td>84.88</td>\n      <td>87.80</td>\n      <td>13.68</td>\n      <td>24.49</td>\n      <td>27.03</td>\n    </tr>\n  </tbody>\n</table>\n`;\n"],"names":["description","content"],"sourceRoot":""}\n//# sourceURL=webpack-internal:///(app-pages-browser)/./src/data/projects/content/llm-coding-eval.ts\n"));

/***/ })

});